# Predicting Future Video Frames using Pred-Net architecture - Project in the Cognitive Systems Master Program

by [Roshan Rane](https://github.com/RoshanRane), [Vageesh Saxena](https://github.com/vageeshSaxena), and [Edit Szugyi](https://github.com/szugyiedit)

## Problem:
Future frame prediction in videos is a promising avenue for unsupervised video representation learning. Video frames are naturally generated by the inherent pixel flows from preceding frames based on the appearance and motion dynamics in the video.

## Solution: 
Our goal in this project is to use various Deep-Learning/ Generative architecures to predict the future frames for a given real-time video. Also, in this project we tried to used the annotated text to evaluate whether it helps to explain the generation of correct future frames or not. 

## Link for benchmark dataset:
https://20bn.com/datasets/something-something/v2

## About dataset(220,847 videos):
1) Height : 240px 
2) Width : Variable
3) FPS : 12  
4) Training : 168,913 videos
5) Validation : 24,777 videos
6) Test : 27,157 videos
7) Labels: 
  A) 50 coarse-grained action groups.
  B) 174 action categories e.g."Putting [something] onto [something]"  
  C) 318,572 annotations and 30,408 unique objects e.g. â€œPutting a blue cup onto a table"  

## Link to Pred-net implementations in Keras:
1) https://coxlab.github.io/prednet/
2) https://github.com/coxlab/prednet
3) https://github.com/kunimasa-kawasaki/keras-prednet

## Links to Pred-net implementations in pytorch:
1) https://github.com/leido/pytorch-prednet
2) https://github.com/bionick87/PredNet-ConvGRU
3) https://github.com/Atcold/pytorch-CortexNet/blob/master/model/PrednetModel.py

## Links to Pred-net implementations in Tensorflow:
1) https://github.com/kikyou123/prednet_tf

## Benchmark Model:
1) https://arxiv.org/abs/1605.08104
2) https://github.com/TwentyBN/something-something-v2-baseline
